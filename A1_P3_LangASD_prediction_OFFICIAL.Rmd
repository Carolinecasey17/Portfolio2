---
title: "Assignment 1 - Language Development in ASD - part 3"
author: "Riccardo Fusaroli"
date: "August 10, 2017"
output: word_document
---

```{r setup, echo=F}
knitr::opts_chunk$set(echo = TRUE)

```

## Welcome to the third exciting part of the Language Development in ASD exercise

In this exercise we will delve more in depth with different practices of model comparison and model selection, by first evaluating your models from last time, then learning how to cross-validate models and finally how to systematically compare models.

N.B. There are several datasets for this exercise, so pay attention to which one you are using!

1. The (training) dataset from last time (the awesome one you produced :-) ).
2. The (test) datasets on which you can test the models from last time:
* Demographic and clinical data: https://www.dropbox.com/s/ra99bdvm6fzay3g/demo_test.csv?dl=0
* Utterance Length data: https://www.dropbox.com/s/uxtqqzl18nwxowq/LU_test.csv?dl=0
* Word data: https://www.dropbox.com/s/1ces4hv8kh0stov/token_test.csv?dl=0

### Exercise 1) Testing model performance

How did your models from last time perform? In this exercise you have to compare the results on the training data () and on the test data. Report both of them. Compare them. Discuss why they are different.

- recreate the models you chose last time (just write the code again and apply it to Assignment2TrainData1.csv)
- calculate performance of the model on the training data: root mean square error is a good measure. (Tip: google the functions rmse() and predict() ) (this in our loop)
- create the test dataset (apply the code from assignment 1 part 1 to clean up the 3 test datasets) (done)
- test the performance of the models on the test data (Tips: time to reuse "predict()")
- optional: predictions are never certain, can you identify the uncertainty of the predictions? (e.g. google predictinterval())


```{r}
library(pacman)
#p_load both load the libraries and install packages
p_load(lmerTest, lme4, ggplot2, pastecs, Metrics, MuMIn, merTools, groupdata2, stringr, dplyr)

setwd("~/Documents/RStudioDocs/Assignment 1")

# LOAD DATA
#old training data load
traindata = read.csv("AutismWithAvg.csv", sep = ",")
traindata$X=NULL
traindata$ID=as.numeric(traindata$ID)
traindata$Visit=as.numeric(traindata$Visit)

## Recreate models from last time & performance 
cltmodel5 = lmer(CHI_MLU ~ Visit + verbalIQ + MOT_MLU + (1 + Visit|ID), data = traindata, REML = F)
summary(cltmodel5)

## calculate performance 
sim_train = fitted(cltmodel5)
rmse(sim_train, traindata$CHI_MLU)

#load new data
demo_test = read.csv("demo_test.csv", sep = ",")
LU_test = read.csv("LU_test.csv", sep = ",")
token_test = read.csv("token_test.csv", sep =",")

# CLEAN DATA
#making "Visit" the consistent name
names(LU_test)[names(LU_test)=="VISIT"]="Visit"
names(token_test)[names(token_test)=="VISIT"]="Visit"

#making ID the consistent name
names(token_test)[names(token_test)=="SUBJ"]="ID"
names(demo_test)[names(demo_test)=="Child.ID"]="ID"
names(LU_test)[names(LU_test)=="SUBJ"]="ID"


LU_test$Visit=str_extract(LU_test$Visit, "\\d")
token_test$Visit=str_extract(token_test$Visit, "\\d")

#using gsub to remove all dots
LU_test$ID=gsub("\\.", "", LU_test$ID)
demo_test$ID=gsub("\\.", "", demo_test$ID)
token_test$ID=gsub("\\.", "", token_test$ID)

# RENAME 

# using select to create subsets of the data containing only the wanted variables.
demo_test_sub = dplyr::select(demo_test, ID, Visit, Ethnicity, Diagnosis, Gender, Age, ADOS, MullenRaw, ExpressiveLangRaw)

LU_test_sub = dplyr::select(LU_test, ID, Visit, MOT_MLU, MOT_LUstd, CHI_MLU, CHI_LUstd)

token_test_sub = dplyr::select(token_test, ID, Visit, types_MOT, types_CHI, tokens_MOT, tokens_CHI)

# rename verbalIQ and nonverbalIQ
names(demo_test_sub)[names(demo_test_sub)=="MullenRaw"]="nonVerbalIQ"
names(demo_test_sub)[names(demo_test_sub)=="ExpressiveLangRaw"]="verbalIQ"

# merge data
data1=merge(LU_test_sub,token_test_sub,by=c("ID","Visit"))
testdata=merge(data1,demo_test_sub,by=c("ID","Visit")) 

# Creating subset with only first visit
visit1 = subset(testdata[testdata$Visit == "1",])

#Selecting only relevant variables
relevant_data=dplyr::select(visit1,ADOS,verbalIQ,nonVerbalIQ,ID)

testdata=testdata[-15:-17]

# Merging the two datasets
testdata = merge(testdata, relevant_data, by ="ID")

# make as factor
testdata$ID=as.factor(testdata$ID)


# Renaming the levels as 1 through lengths of levels
levels(testdata$ID)=1:length(levels(testdata$ID))

# make visit as numeric
testdata$Visit=as.numeric(testdata$Visit)


# Rename gender
testdata$Gender=as.factor(testdata$Gender)
testdata$Gender = recode(testdata$Gender, "1"="M", "2" ="F")

testdata$Diagnosis= recode(testdata$Diagnosis,"A"="ASD","B"="TD")

## Excercise 1d, performance of models on test data 
sim_test = predict(cltmodel5, testdata)
rmse(sim_test, testdata$CHI_MLU)


```

Performance of training data = 0.34
Performance of test data = 0.70 

### Exercise 2) Model Selection via Cross-validation (N.B: ChildMLU!)

One way to reduce bad surprises when testing a model on new data is to train the model via cross-validation. 

In this exercise you have to use cross-validation to calculate the predictive error of your models and use this predictive error to select the best possible model.

- Create the basic model of ChildMLU as a function of Time and Diagnosis (don't forget the random effects!).
- Make a cross-validated version of the model. (Tips: google the function "createFolds";  loop through each fold, train a model on the other folds and test it on the fold)
- Report the results and comment on them.

- Now try to find the best possible predictive model of ChildMLU, that is, the one that produces the best cross-validated results.

- Bonus Question 1: How would you go comparing the performance of the basic model and the cross-validated model on the testing set?
- Bonus Question 2: What is the effect of changing the number of folds? Can you plot RMSE as a function of number of folds?
- Bonus Question 3: compare the cross-validated predictive error against the actual predictive error on the test data


```{r}

traindata = fold(traindata, k=4, cat_col = "Diagnosis", id_col = "ID")

#Creating function for automatic crossvalidation. Outputs R2c, R2m and RMSE for each fold, as well the mean values across folds

cv = function(data, k, model, dependent){
#Creating variables for storing performances
rmselist = list() # saving the values we get when we test them 
r2list = list() # saving r-squared values when we test them 
#Creating loop
for (i in 1:k){
  train = data[data$.folds != i,]    #creating training set (all folds except the one)
  validation = data[data$.folds == i,] #creating testing/validation set (the current fold)
  model = lmer(model, train, REML = F)   #running lmer on the model specified in the function call
  rmselist[i] = Metrics::rmse(validation[[dependent]], predict(model, validation, allow.new.levels = T))  #saving model rmse, 
  r2list[i] = as.data.frame(r.squaredGLMM(model))    #saving r2c and r2m values for the model 
}
#doing some wrangling so the R2 outputs can be printed in a nice format
r2list = as.data.frame(t(as.data.frame(r2list)))
colnames(r2list) = c("R2m", "R2c")
rownames(r2list) = seq(1:k)
r2list = as.data.frame(r2list)

#returning the wanted values
return(c('RMSE' = rmselist, 'Mean RMSE' = mean(unlist(rmselist)), r2list,  'Mean R2m' = mean(r2list$R2m), 'Mean R2c' =  mean(r2list$R2c)))
}


m1 = "CHI_MLU ~ Visit + Diagnosis + (1+Visit|ID)" 
cv(traindata, 4, m1, 'CHI_MLU')

# Make more models to compare and run through loop

m2 = "CHI_MLU ~ Visit + Diagnosis + MOT_MLU+ (1+Visit|ID)"
cv(traindata, 4, m2, 'CHI_MLU')

m3 = "CHI_MLU ~ Diagnosis*Visit + (1+Visit|ID)"
cv(traindata, 4, m3, 'CHI_MLU')

m4 = "CHI_MLU ~ Visit + Diagnosis + verbalIQ + (1+Visit|ID)"
cv(traindata, 4, m4, 'CHI_MLU')

m5 = "CHI_MLU ~ Visit + MOT_MLU + verbalIQ + (1 + Visit|ID)"
cv(traindata, 4, m5, 'CHI_MLU')



## Best to predict with inclusion of tokens and types, but that isn't "fair", so model 5 is the best, with a mean R2m of 0.57. 

```

Model 5 is the model that fits best with the predictive model 


### Exercise 3) Assessing the single child

Let's get to business. This new kiddo - Bernie - has entered your clinic. This child has to be assessed according to his group's average and his expected development.

Bernie is one of the six kids in the test dataset, so make sure to extract that child alone for the following analysis.

You want to evaluate:

- how does the child fare in ChildMLU compared to the average TD child at each visit? Define the distance in terms of absolute difference between this Child and the average TD.
(Tip: recreate the equation of the model: Y=Intercept+BetaX1+BetaX2, etc; input the average of the TD group  for each parameter in the model as X1, X2, etc.).

- how does the child fare compared to the model predictions at Visit 6? Is the child below or above expectations? (tip: use the predict() function on Bernie's data only and compare the prediction with the actual performance of the child)

```{r}
#bernie subset
bernie=subset(testdata,ID=="2")

berniedata = lmer(CHI_MLU ~ Visit + MOT_MLU + verbalIQ + (1 + Visit|ID), traindata)
summary(berniedata)

#subset over traindata with only Visit 1 
tsubset = subset(traindata, Visit =="1")
#subset over visit1subset with only TD children
tsub=subset(traindata, Diagnosis=="TD")

# VISIT 1 MEAN CHI_MLU
mean(tsub$CHI_MLU)  #typically developing child in visit 1, CHI_MLU = 1.31
bernie$CHI_MLU[1]  # bernie's mean, CHI_MLU = 1.98. Better than typically developed child. 
mean(tsub$CHI_MLU)-bernie$CHI_MLU[1] # Bernie produces 0.67 CHI_MLU more than TD children. 

# LOOP THIS SHIT 

avgMOTMLU = tsub %>% 
  group_by(Visit) %>% 
  summarize(MOT_MLU = mean(MOT_MLU))

avgCHI_MLU = tsub %>%
  group_by(Visit) %>%
  summarize(CHI_MLU = mean(CHI_MLU))
avgMOTMLU[2]
avgCHI_MLU

# To see the difference between Bernie and TD children across all visits.  
bernie$CHI_MLU - avgCHI_MLU

# dataframe
td_chi = data.frame(ID = rep(300, 6), Visit = seq(1,6), Diagnosis = "TD", verbalIQ = mean(tsub$verbalIQ), MOT_MLU = avgMOTMLU[2], CHI_MLU = avgCHI_MLU[2])

bernie = dplyr::select(bernie, ID, Visit, Diagnosis, verbalIQ, MOT_MLU,CHI_MLU)


# bind it together
avgTD_bernie = rbind(td_chi, bernie)
avgTD_bernie$ID = as.factor(avgTD_bernie$ID)

#plot CHI_MLU and Visit (Bernie data )
ggplot(avgTD_bernie, aes(Visit, CHI_MLU, color = ID)) +
  geom_point() +
  geom_line()

# To see the TD children avg through visits 
td_chi

# mean verbalIQ for TD = 20.15 
mean(tsub$verbalIQ)

# PREDICT BERNIE
#use predict function to predict bernie... 
predict(berniedata,bernie, allow.new.levels = T)

predictbernie = bernie
predictbernie$CHI_MLU = predict(berniedata, bernie, allow.new.levels = T)
predictbernie$ID = rep("predictbernie", 6)

avgTD_bernie = rbind(avgTD_bernie, predictbernie)

# Visit 6 - Ask about this 
B6 = subset(bernie, Visit ==6)
pred = predict(berniedata, B6)
B6$CHI_MLU - pred

#plot predicted bernie vs real bernie 
ggplot(avgTD_bernie, aes(Visit, CHI_MLU, color = ID)) +
  geom_point()+
  geom_line()

```



